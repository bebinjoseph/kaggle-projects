library(tidyverse)
install.packages(tidyverse)
help tidyverse
help
install.packages("tidyverse")
library(tidyverse)
install.packages("openintro")
library(openintro)
glimpse(starwars)
install.packages("shiny")
library(shiny)
runExample("06_tabsets")
library(fontawesome)
# With one input
calc_sample_mean <- function(sample_size, our_mean=0, our_sd=1) {
sample <- rnorm(sample_size,
mean = our_mean,
mean(sample)
}
# With one input
calc_sample_mean <- function(sample_size, our_mean=0, our_sd=1) {
sample <- rnorm(sample_size,
mean = our_mean,
sd= our_sd
mean(sample)
# With one input
calc_sample_mean <- function(sample_size, our_mean=0, our_sd=1) {
sample <- rnorm(sample_size,
mean = our_mean,
sd= our_sd)
mean(sample)
}
# With vector input
calc_sample_mean(5)
# With vector input
calc_sample_mean(5)
# With vector input
calc_sample_mean(c(1,2,3,5))
?ggplot
library(tidyverse)
?ggplot
?scale_x_discrete
?ylim
library(tidyverse)
?airquality
knitr::opts_chunk$set(echo = TRUE)
airquality
data=airquality
glimpse(data)
ggplot(data)
?ggplot
ggplot(data, aes(x = Month, y = Ozone)) +
geom_point() +
labs(x = "Month", y = "Ozone") +
ggtitle("Column Ozone vs. Column Month in airquality dataset")
ggplot(data, aes(x = Month, y = Ozone)) +
geom_point() +
labs(x = "Month", y = "Ozone") +
ggtitle("Column Ozone vs. Column Month in airquality dataset") + ylim(100,150)
ggplot(data, aes(x = Month, y = Ozone)) +
geom_point() +
labs(x = "Month", y = "Ozone") +
ggtitle("Column Ozone vs. Column Month in airquality dataset") + ylim(50,100)
ggplot(data, aes(x = Month, y = Ozone)) +
geom_point() +
labs(x = "Month", y = "Ozone") +
ggtitle("Column Ozone vs. Column Month in airquality dataset")
ggplot(data, aes(x = Month, y = Ozone)) +
geom_point() +
labs(x = "Month", y = "Ozone") +
ggtitle("Column Ozone vs. Column Month in airquality dataset") + ylim(0,50)
ggplot(data, aes(x = Month, y = Ozone)) +
geom_point() +
labs(x = "Month", y = "Ozone") +
ggtitle("Column Ozone vs. Column Month in airquality dataset") + ylim(500,1000)
ggplot(data, aes(x = Month, y = Ozone)) +
geom_point() +
labs(x = "Month", y = "Ozone") + ylim(0,50) +
ggtitle("Column Ozone vs. Column Month in airquality dataset")
ggplot(data, aes(x = Month, y = Ozone)) +
geom_point() +
labs(x = "Month", y = "Ozone") +
ggtitle("Column Ozone vs. Column Month in airquality dataset")
ggplot(data, aes(x = Month, y = Ozone/10)) +
geom_point() +
labs(x = "Month", y = "Ozone") +
ggtitle("Column Ozone vs. Column Month in airquality dataset")
ggplot(data, aes(x = Month, y = Ozone/10)) +
geom_point() +
labs(x = "Month", y = "Ozone") + ylim(10,15) +
ggtitle("Column Ozone vs. Column Month in airquality dataset")
ggplot(data, aes(x = Ozone)) +
geom_histogram(binwidth = 10, fill = "blue", color = "black") +
labs(x = "Ozone", y = "Frequency") +
ggtitle("Histogram of Ozone in airquality dataset")
ggplot(data, aes(x = Ozone)) +
geom_histogram(binwidth = 10, fill = "blue", color = "black") +
labs(x = "Ozone", y = "Frequency") +
ggtitle("Histogram of Ozone in airquality dataset") + ylim(10,50)
ggplot(data, aes(x = Ozone)) +
geom_histogram(binwidth = 10, fill = "blue", color = "black") +
labs(x = "Ozone", y = "Frequency") +
ggtitle("Histogram of Ozone in airquality dataset")
ggplot(data, aes(x = Ozone)) +
geom_histogram(binwidth = 10, fill = "blue", color = "black") +
labs(x = "Ozone", y = "Frequency") +
ggtitle("Histogram of Ozone in airquality dataset") + coord_cartesian(ylim = c(10,50))
library(shiny)
knitr::opts_chunk$set(echo = TRUE)
runExample(1)
?runExample
knitr::opts_chunk$set(echo = TRUE)
runExample("01_hello")
library(shiny)
runExample("01_hello")
install.packages(c("archive", "brio", "bslib", "cli", "cluster", "cpp11", "crosstalk", "curl", "data.table", "DBI", "desc", "dplyr", "e1071", "fansi", "foreign", "gert", "haven", "htmlwidgets", "httpuv", "httr2", "jsonlite", "later", "lattice", "leaflet", "lifecycle", "markdown", "Matrix", "mgcv", "nlme", "processx", "progress", "ragg", "rlang", "rnaturalearth", "rpart", "rprojroot", "rsconnect", "s2", "sass", "scales", "sf", "shiny", "shinylive", "sp", "stringi", "stringr", "terra", "tinytex", "units", "vctrs", "vroom", "wk", "xml2", "yaml"))
View(calc_sample_mean)
gc()
View(calc_sample_mean)
rm(calc_sample_mean())
remove(calc_sample_mean
)
install.packages("stats")
install.packages("readr")
install.packages("pracma")
install.packages("stats")
install.packages("readr")
install.packages("stats")
install.packages("readr")
install.packages("pracma")
install.packages("stats")
devtools::install_github("cykbennie/fbi")
library(eFRED)
library(tidyverse)
library(fredr)
#fredr_set_key("75b9d507a06d8895301b8c193c93e4ae")
merged_data<-read_csv("mergedData.csv")
herbData<-read_csv("ICSA.csv")
postCov<-read_csv("post covid 20-23.csv")
View(merged_data)
View(merged_data)
test<-merged_data
?drop
setwd("~/Documents/Github/kaggle-projects/House Price")
# Load necessary libraries
#library(caret) # for data splitting
rm(list = ls())
# Load your own training and test datasets
train_data <- read.csv("train.csv")
test_data <- read.csv("test.csv")
newtrain<-train_data%>%select(where(is.numeric))
library(tidyverse)
newtrain<-train_data%>%select(where(is.numeric))
#remove all objects from memory
rm(list = ls())
#####################################################################
#Preliminary steps: load data, check for missing values etc.
#####################################################################
#Load the ISLR library which contains the dataset we will use:
library(ISLR)
attach(Credit)
#Display the variable names that are in the dataset:
names(Credit)
sum(is.na(Credit)) #is.na() returns a logical TRUE/FALSE value for whether an entry is a NaN or not
dim(Credit) #check dataset size: we have N=400 for 12 variables
library(leaps)
library(tictoc) #this library will help us measure computation time conveniently
#Export the data into a dataframe:
df=Credit
#Set the training set size:
ntrain=300
set.seed(2457829) #seed of the random number generator for replicability
tr = sample(1:nrow(df),ntrain)  # randomly sampled ntrain indices of observations
train = df[tr,]   # get the training sample
test = df[-tr,]   # get the testing sample
View(test)
View(train)
#Placing tic() at the start and toc() at the end of a block of code will help us measure
#how long it takes to execute it. This may be helpful when number of predictors grows.
tic()
#Note: regsubsets will automatically convert categorical variables (like Student, Ethnicity, Married) into appropriate (groups of) dummy variables
bssel = regsubsets(Balance~.-ID, data = train, nvmax = 11, method = "exhaustive")
toc()
#Summary of the results
sumbss = summary(bssel)
View(sumbss)
?regsubsets
View(sumbss)
View(bssel)
# Load necessary libraries
#library(caret) # for data splitting
library(leaps)
rm(list = ls())
# Load your own training and test datasets
train_data <- read.csv("train.csv")
test_data <- read.csv("test.csv")
# Fit OLS model on the training data
#init_ols_model <- lm(SalePrice ~ LotArea, data = train_data)
tic()
fwsel = regsubsets(Balance~.-ID, data = train, nvmax = 11, method = "forward")
fwsel = regsubsets(Balance~.-ID, data = train_data, nvmax = 20, method = "forward")
View(train_data)
fwsel = regsubsets(SalePrice~.-Id, data = train_data, nvmax = 20, method = "forward")
newtrain<-train_data%>%select(where(is.numeric))
View(newtrain)
fwsel = regsubsets(SalePrice~.-Id, data = newtrain, nvmax = 15, method = "forward")
toc()
####
tic()
# Check for collinearity
correlation_matrix <- cor(newtrain[, -which(names(newtrain) == "Id")])  # Remove 'Id' column for correlation calculation
highly_correlated <- findCorrelation(correlation_matrix, cutoff = 0.8)
# Print highly correlated variables
cat("Highly correlated variables:\n")
# Load necessary libraries
library(caret) # for data splitting
install.packages("caret")
# Load necessary libraries
library(caret) # for data splitting
# Check for collinearity
correlation_matrix <- cor(newtrain[, -which(names(newtrain) == "Id")])  # Remove 'Id' column for correlation calculation
highly_correlated <- findCorrelation(correlation_matrix, cutoff = 0.8)
library(caret)
# Compute correlation matrix with NA handling
correlation_matrix <- cor(newtrain[, -which(names(newtrain) == "Id")], use = "pairwise.complete.obs")
# Find highly correlated variables
highly_correlated <- findCorrelation(correlation_matrix, cutoff = 0.8, verbose = TRUE)
# Print highly correlated variables
cat("Highly correlated variables:\n")
print(names(newtrain)[highly_correlated])
# Remove highly correlated variables from the dataset
newtrain_clean <- newtrain[, -highly_correlated]
# Re-run subset selection
fwsel <- regsubsets(SalePrice ~ . - Id, data = newtrain_clean, nvmax = 15, method = "forward")
View(fwsel)
toc()
sumbfs = summary(fwsel)
#print results
sumbfs
View(sumbfs)
View(fwsel)
kbicf=which.min(sumbfs$bic) #BIC choice
kaicf=which.min(sumbfs$cp)  #AIC choice (AIC proportional to Cp and so ranking is the same)
which.max(sumbfs$adjr2) #for kicks, see the model with the highest adjusted R-squared - does it overfit?
plot((sumbfs$bic-mean(sumbfs$bic))/sd(sumbfs$bic), main = "AIC and BIC (Standardized)", xlab = "k",
ylab = "IC", type = "l", col = "blue")
points((sumbfs$cp-mean(sumbfs$cp))/sd(sumbfs$cp), type = "l", col = "red")
legend("topright", c("BIC","AIC"),lty=c(1,1) ,col=c("blue","red"))
?predict
View(sumbfs)
View(fwsel)
#extract coefficients from the best model on BIC
temp.coef = coef(fwsel, id = kbicf)
# Get the selected variables from the forward selection result
selected_vars <- names(coef(fwsel, id = 15))
# Fit the regression model using the selected variables
final_model <- lm(SalePrice ~ . - Id, data = newtrain_clean[, c("SalePrice", selected_vars)])
selected_vars
# Fit the regression model using the selected variables
final_model <- lm(SalePrice ~ MSSubClass, LotArea, data = newtrain_clean
# Fit the regression model using the selected variables
final_model <- lm(SalePrice ~ MSSubClass, LotArea, data = newtrain_clean)
# Fit the regression model using the selected variables
final_model <- lm(SalePrice ~ MSSubClass, LotArea, data = newtrain_clean)
selected_vars
# Fit the regression model using the selected variables
final_model <- lm(SalePrice ~ MSSubClass, LotArea, OverallQual, YearBuilt, YearRemodAdd, MasVnrArea, BsmtFinSF1, GrLivArea, BsmtFullBath, BedroomAbvGr, TotRmsAbvGrd, GarageCars, WoodDeckSF, ScreenPorch, PoolArea, data = newtrain_clean)
View(newtrain_clean)
View(newtrain)
# Fit the regression model using the selected variables
final_model <- lm(SalePrice ~ MSSubClass, LotArea, OverallQual, YearBuilt, YearRemodAdd, MasVnrArea, BsmtFinSF1, GrLivArea, BsmtFullBath, BedroomAbvGr, TotRmsAbvGrd, GarageCars, WoodDeckSF, ScreenPorch, PoolArea, data = newtrain_clean)
# Fit the regression model using the selected variables
final_model <- lm(SalePrice ~ MSSubClass, LotArea, OverallQual, YearBuilt, YearRemodAdd, MasVnrArea,BsmtFinSF1, GrLivArea, BsmtFullBath, BedroomAbvGr, TotRmsAbvGrd, GarageCars, WoodDeckSF, ScreenPorch, PoolArea, data = newtrain_clean)
View(final_model)
View(final_model)
names(newtrain_clean)
# Fit the regression model using the selected variables
final_model <- lm(SalePrice ~ MSSubClass, LotArea, OverallQual, YearBuilt, YearRemodAdd, MasVnrArea, BsmtFinSF1, GrLivArea, BsmtFullBath, BedroomAbvGr, TotRmsAbvGrd, GarageCars, WoodDeckSF, ScreenPorch, PoolArea, data = newtrain_clean)
# Fit the regression model using the selected variables
# Fit the regression model using the selected variables
final_model <- lm(SalePrice ~ MSSubClass + LotArea + OverallQual + YearBuilt + YearRemodAdd + MasVnrArea + BsmtFinSF1 + GrLivArea + BsmtFullBath + BedroomAbvGr + TotRmsAbvGrd + GarageCars + WoodDeckSF + ScreenPorch + PoolArea, data = newtrain_clean)
View(final_model)
# Summarize the final model
summary(final_model)
fssel_test_pred<- predict(final_model, newdata = test_data)
fssel_test_pred_df<- data.frame(SalePrice=fssel_test_pred)
View(fssel_test_pred_df)
submission3<-cbind(test_data$Id,fssel_test_pred_df)
View(submission3)
names(submission3)[names(submission3) == "test_data$Id"] <- "Id"
View(submission3)
View(submission3)
write.csv(submission3, "submission3forwardselection.csv", row.names = FALSE)
is.na(submission3)
sum(is.na(submission3))
sum(is.na(test_data))
sum(is.na(test_data[selected_vars]))
sum(is.na(newtrain[selected_vars]))
sum(is.na(newtrain_clean[selected_vars]))
selected_vars
typeof(selected_vars)
list(selected_vars)
typeof(selected_vars)
typeof(list(selected_vars))
selvar=list(selected_vars)
View(selvar)
selvar=data.frame(selected_vars)
sum(is.na(newtrain_clean[selvar]))
typeof(selvar)
# Define the variable names
selva <- c("MSSubClass", "LotArea", "OverallQual", "YearBuilt", "YearRemodAdd",
"MasVnrArea", "BsmtFinSF1", "GrLivArea", "BsmtFullBath", "BedroomAbvGr",
"TotRmsAbvGrd", "GarageCars", "WoodDeckSF", "ScreenPorch", "PoolArea")
# Subset test_dat using the selected variables
test_subset <- test_data[, selva]
sum(is.na(test_subset))
View(test_subset)
View(submission3)
# Calculate the mean SalePrice
mean_saleprice <- mean(submission3$SalePrice, na.rm = TRUE)
# Impute missing SalePrice values with the mean
submission3$SalePrice[is.na(submission3$SalePrice)] <- mean_saleprice
sum(is.na(submission3))
View(submission3)
write.csv(submission3, "submission3forwardselection.csv", row.names = FALSE)
rm(list = ls())
