library(tidyverse)
install.packages(tidyverse)
help tidyverse
help
install.packages("tidyverse")
library(tidyverse)
install.packages("openintro")
library(openintro)
glimpse(starwars)
install.packages("shiny")
library(shiny)
runExample("06_tabsets")
library(fontawesome)
# With one input
calc_sample_mean <- function(sample_size, our_mean=0, our_sd=1) {
sample <- rnorm(sample_size,
mean = our_mean,
mean(sample)
}
# With one input
calc_sample_mean <- function(sample_size, our_mean=0, our_sd=1) {
sample <- rnorm(sample_size,
mean = our_mean,
sd= our_sd
mean(sample)
# With one input
calc_sample_mean <- function(sample_size, our_mean=0, our_sd=1) {
sample <- rnorm(sample_size,
mean = our_mean,
sd= our_sd)
mean(sample)
}
# With vector input
calc_sample_mean(5)
# With vector input
calc_sample_mean(5)
# With vector input
calc_sample_mean(c(1,2,3,5))
?ggplot
library(tidyverse)
?ggplot
?scale_x_discrete
?ylim
library(tidyverse)
?airquality
knitr::opts_chunk$set(echo = TRUE)
airquality
data=airquality
glimpse(data)
ggplot(data)
?ggplot
ggplot(data, aes(x = Month, y = Ozone)) +
geom_point() +
labs(x = "Month", y = "Ozone") +
ggtitle("Column Ozone vs. Column Month in airquality dataset")
ggplot(data, aes(x = Month, y = Ozone)) +
geom_point() +
labs(x = "Month", y = "Ozone") +
ggtitle("Column Ozone vs. Column Month in airquality dataset") + ylim(100,150)
ggplot(data, aes(x = Month, y = Ozone)) +
geom_point() +
labs(x = "Month", y = "Ozone") +
ggtitle("Column Ozone vs. Column Month in airquality dataset") + ylim(50,100)
ggplot(data, aes(x = Month, y = Ozone)) +
geom_point() +
labs(x = "Month", y = "Ozone") +
ggtitle("Column Ozone vs. Column Month in airquality dataset")
ggplot(data, aes(x = Month, y = Ozone)) +
geom_point() +
labs(x = "Month", y = "Ozone") +
ggtitle("Column Ozone vs. Column Month in airquality dataset") + ylim(0,50)
ggplot(data, aes(x = Month, y = Ozone)) +
geom_point() +
labs(x = "Month", y = "Ozone") +
ggtitle("Column Ozone vs. Column Month in airquality dataset") + ylim(500,1000)
ggplot(data, aes(x = Month, y = Ozone)) +
geom_point() +
labs(x = "Month", y = "Ozone") + ylim(0,50) +
ggtitle("Column Ozone vs. Column Month in airquality dataset")
ggplot(data, aes(x = Month, y = Ozone)) +
geom_point() +
labs(x = "Month", y = "Ozone") +
ggtitle("Column Ozone vs. Column Month in airquality dataset")
ggplot(data, aes(x = Month, y = Ozone/10)) +
geom_point() +
labs(x = "Month", y = "Ozone") +
ggtitle("Column Ozone vs. Column Month in airquality dataset")
ggplot(data, aes(x = Month, y = Ozone/10)) +
geom_point() +
labs(x = "Month", y = "Ozone") + ylim(10,15) +
ggtitle("Column Ozone vs. Column Month in airquality dataset")
ggplot(data, aes(x = Ozone)) +
geom_histogram(binwidth = 10, fill = "blue", color = "black") +
labs(x = "Ozone", y = "Frequency") +
ggtitle("Histogram of Ozone in airquality dataset")
ggplot(data, aes(x = Ozone)) +
geom_histogram(binwidth = 10, fill = "blue", color = "black") +
labs(x = "Ozone", y = "Frequency") +
ggtitle("Histogram of Ozone in airquality dataset") + ylim(10,50)
ggplot(data, aes(x = Ozone)) +
geom_histogram(binwidth = 10, fill = "blue", color = "black") +
labs(x = "Ozone", y = "Frequency") +
ggtitle("Histogram of Ozone in airquality dataset")
ggplot(data, aes(x = Ozone)) +
geom_histogram(binwidth = 10, fill = "blue", color = "black") +
labs(x = "Ozone", y = "Frequency") +
ggtitle("Histogram of Ozone in airquality dataset") + coord_cartesian(ylim = c(10,50))
library(shiny)
knitr::opts_chunk$set(echo = TRUE)
runExample(1)
?runExample
knitr::opts_chunk$set(echo = TRUE)
runExample("01_hello")
library(shiny)
runExample("01_hello")
install.packages(c("archive", "brio", "bslib", "cli", "cluster", "cpp11", "crosstalk", "curl", "data.table", "DBI", "desc", "dplyr", "e1071", "fansi", "foreign", "gert", "haven", "htmlwidgets", "httpuv", "httr2", "jsonlite", "later", "lattice", "leaflet", "lifecycle", "markdown", "Matrix", "mgcv", "nlme", "processx", "progress", "ragg", "rlang", "rnaturalearth", "rpart", "rprojroot", "rsconnect", "s2", "sass", "scales", "sf", "shiny", "shinylive", "sp", "stringi", "stringr", "terra", "tinytex", "units", "vctrs", "vroom", "wk", "xml2", "yaml"))
View(calc_sample_mean)
gc()
View(calc_sample_mean)
rm(calc_sample_mean())
remove(calc_sample_mean
)
install.packages("stats")
install.packages("readr")
install.packages("pracma")
install.packages("stats")
install.packages("readr")
install.packages("stats")
install.packages("readr")
install.packages("pracma")
install.packages("stats")
devtools::install_github("cykbennie/fbi")
library(eFRED)
library(tidyverse)
library(fredr)
#fredr_set_key("75b9d507a06d8895301b8c193c93e4ae")
merged_data<-read_csv("mergedData.csv")
herbData<-read_csv("ICSA.csv")
postCov<-read_csv("post covid 20-23.csv")
View(merged_data)
View(merged_data)
test<-merged_data
?drop
library(tidyverse)
setwd("~/Documents/Kaggle/House Price")
# Load your own training and test datasets
train_data <- read.csv("train.csv")
rm()
# Load necessary libraries
#library(caret) # for data splitting
rm(list = ls())
# Load your own training and test datasets
train_data <- read.csv("train.csv")
test_data <- read.csv("test.csv")
View(train_data)
typeof(train_data)
print(typeof(train_data[0]))
train_data[0]
train_data[``]
train_data[1]
head(train_data[2])
head(train_data[1])
head(train_data[0])
print(typeof(train_data[1]))
print(typeof(train_data[2]))
typeof(train_data[5])
class(train_data[3])
# Fit OLS model on the training data
ols_model <- lm(SalePrice ~ ., data = train_data)
lm?
?lm
?lm
# Fit OLS model on the training data
ols_model <- lm(SalePrice ~ LotArea, data = train_data)
# Print the summary of the model
summary(ols_model)
View(ols_model)
View(ols_model)
# Predict on the test data using the trained model
test_predictions <- predict(ols_model, newdata = test_data)
# Calculate metrics (if needed)
# For example, Mean Squared Error (MSE)
mse <- mean((test_data$dependent_variable - test_predictions)^2)
print(paste("Mean Squared Error:", mse))
# Calculate metrics (if needed)
# For example, Mean Squared Error (MSE)
mse <- mean((test_data$SalePrice - test_predictions)^2)
print(paste("Mean Squared Error:", mse))
# Predict on the test data using the trained model
test_predictions <- predict(ols_model, newdata = test_data)
# Calculate metrics (if needed)
# For example, Mean Squared Error (MSE)
mse <- mean((test_data$SalePrice - test_predictions)^2)
print(paste("Mean Squared Error:", mse))
View(test_data)
# Print the summary of the model
summary(ols_model)
# Predict on the test data using the trained model
test_predictions <- predict(ols_model, newdata = test_data)
# Calculate metrics (if needed)
# Calculate metrics (if needed)
# For example, Mean Squared Error (MSE)
# Calculate metrics (if needed)
# For example, Mean Squared Error (MSE)
#mse <- mean((test_data$SalePrice - test_predictions)^2)
test_data1<-test_data.merge(test_predictions)
test_predictions
count(test_predictions)
typeof(test_predictions)
list(test_predictions)
testlist<-list(test_predictions)
View(testlist)
merge(test_data,testlist)
View(test_data)
mergedData<-merge(test_data,test_predictions)
View(mergedData)
clear
rm(mergedData)
predictions_df <- data.frame(Predicted_Values = test_predictions)
View(predictions_df)
rm(testlist)
View(test_data)
predictions_df <- data.frame(SalePrice = test_predictions)
merged_data<- cbind(test_data,predictions_df)
View(merged_data)
submission<-cbind(test_data$Id,predictions_df)
View(submission)
submission %>% rename(test_data$Id = Id)
submission %>% rename('test_data$Id' = 'Id')
submission %>% rename(`test_data$Id` = `Id`)
submission %>% rename(Id = `test_data$Id`)
submission %>% rename(Id = test_data$Id)
names(submission)[names(submission) == "test_data$Id"] <- "Id"
write.csv(submission, "submission1bebin.csv", row.names = FALSE)
# Load necessary libraries
#library(caret) # for data splitting
rm(list = ls())
# Load your own training and test datasets
train_data <- read.csv("train.csv")
test_data <- read.csv("test.csv")
# Fit OLS model on the training data
ols_model <- lm(SalePrice ~ LotArea, data = train_data)
rm(ols_model)
# Fit OLS model on the training data
ols_model <- lm(SalePrice ~ LotArea, data = train_data)
# Print the summary of the model
summary(ols_model)
View(test_data)
View(train_data)
typeof(test_data$Id)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
newdata<-data.frame()
for (i in 1:ncol(test_data)) {
}
View(newdata)
newdata<-data.frame()
for (i in 1:ncol(test_data)) {
print(i)
}
test_data(1)
test_data[1]
head(test_data[1])
col(test_data[1])
head(col(test_data[1]))
?col
?colnames
colnames(test_data)
colnames(test_data[2])
colnames(test_data[1])
test_data$colnames(test_data[1])
cname<-colnames(test_data[i])
cname<-colnames(test_data[1])
test_data$cname
test_data$Id
newtrain<-train_data%>%select(where(is.numeric()))
newtrain<-train_data%>%select(where(is.numeric))
View(newtrain)
View(newtrain)
?is.
?is.numeric
?is
oppnewtrain<-train_data%>%select(where(!is.numeric))
oppnewtrain<-train_data%>%select(where(is.numeric!))
oppnewtrain<-train_data%>%select(where(is!.numeric))
oppnewtrain<-train_data%>%select(where((is.numeric)=FALSE))
View(newtrain)
num_ols_model<- lm(SalePrice ~ .-Id, data = newtrain)
View(num_ols_model)
num_test_pred<- predict(num_ols_model, newdata = test_data)
num_test_pred_df<- data.frame(SalePrice=num_test_pred)
#merged_data<- cbind(test_data,predictions_df)
submission<-cbind(test_data$Id,num_test_pred_df)
names(submission)[names(submission) == "test_data$Id"] <- "Id"
write.csv(submission, "submission2bebin.csv", row.names = FALSE)
# Load necessary libraries
#library(caret) # for data splitting
rm(list = ls())
# Load necessary libraries
#library(caret) # for data splitting
rm(list = ls())
# Load your own training and test datasets
train_data <- read.csv("train.csv")
test_data <- read.csv("test.csv")
newtrain<-train_data%>%select(where(is.numeric))
library(tidyverse)
newtrain<-train_data%>%select(where(is.numeric))
num_ols_model<- lm(SalePrice ~ .-Id, data = newtrain)
View(newtrain)
write.csv(newtrain, "newtraindata.csv", row.names = FALSE)
